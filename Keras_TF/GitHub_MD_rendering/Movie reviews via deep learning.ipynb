{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** Movie reviews via deep learning\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or \n",
    "time and the task is to predict a category for the sequence. What makes this problem di\u0000cult is that the\n",
    "sequences can vary in length, be comprised of a very large vocabulary of input symbols and may require the\n",
    "model to learn the long term context or dependencies between symbols in the input sequence\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers import MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 0.4641 - accuracy: 0.7767\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 133s 341ms/step - loss: 0.2787 - accuracy: 0.8889\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 125s 319ms/step - loss: 0.2332 - accuracy: 0.9095\n",
      "Accuracy: 87.22%\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation= \"sigmoid\"))\n",
    "model.compile(loss= \"binary_crossentropy\" , optimizer= \"adam\" , metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs = 3, batch_size=64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can see that this simple LSTM with little tuning achieves near state-of-the-art results on the IMDB problem.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM For Sequence Classification With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Recurrent Neural networks like LSTM generally have the problem of overfitting. Dropout can be applied between \n",
    "layers using\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 130s 332ms/step - loss: 0.5038 - accuracy: 0.7445\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 125s 319ms/step - loss: 0.3159 - accuracy: 0.8725\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 127s 325ms/step - loss: 0.2646 - accuracy: 0.8964\n",
      "Accuracy: 85.84%\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(loss= \"binary_crossentropy\" , optimizer= \"adam\" , metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs = 3, batch_size = 64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can see dropout having the DESIDER IMPACT on training with a slightly slower trend in convergence and in \n",
    "this case a lower final accuracy. The model could probably use a few more epochs of training and may achieve \n",
    "a higher skill (try it an see). Alternately, dropout can be applied to the input and recurrent connections of\n",
    "the memory units with the LSTM precisely and separately.\n",
    "\n",
    "\n",
    "Keras provides this capability with parameters on the LSTM layer, the dropout W for configuring the input \n",
    "dropout and dropout U for configuring the recurrent dropout. For example, we can modify the first example \n",
    "to add dropout to the input and recurrent connections as follows:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 245s 627ms/step - loss: 0.4882 - accuracy: 0.7618\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 246s 629ms/step - loss: 0.2974 - accuracy: 0.8807\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 240s 614ms/step - loss: 0.2801 - accuracy: 0.8879\n",
      "Accuracy: 86.79%\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "# old API --->>> model.add(LSTM(100, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(loss= \"binary_crossentropy\" , optimizer= \"adam\" , metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs = 3, batch_size = 64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ATTENTION: please check with the new API that everything is OK!\n",
    "\n",
    "We can see that the LSTM specific dropout has a more pronounced e↵ect on the convergence of the network than \n",
    "the layer-wise dropout. As above, the number of epochs was kept constant and could be increased to see if the\n",
    "skill of the model can be further lifted. Dropout is a powerful technique for combating overfitting in your \n",
    "LSTM models and it is a good idea to try both methods, \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM and CNN For Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convolutional neural networks excel at learning the spatial structure in input data. The IMDB review data does \n",
    "have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out\n",
    "invariant features for good and bad sentiment. This learned spatial features may then be learned as sequences \n",
    "by an LSTM layer. We can easily add a one-dimensional CNN and max pooling layers after the Embedding layer which\n",
    "then feed the consolidated features to the LSTM\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 73s 186ms/step - loss: 0.4315 - accuracy: 0.7832\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 74s 190ms/step - loss: 0.2391 - accuracy: 0.9061\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 73s 188ms/step - loss: 0.2014 - accuracy: 0.9226\n",
      "Accuracy: 87.78%\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "#model.add(Convolution1D(nb_filter=32, filter_length=3, padding = \"same\", activation= \"relu\"))\n",
    "model.add(Convolution1D(filters = 32, kernel_size = 3, padding = \"same\", activation= \"relu\"))\n",
    "\n",
    "#model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation= \"sigmoid\"))\n",
    "model.compile(loss= \"binary_crossentropy\" , optimizer= \"adam\" , metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs = 3, batch_size = 64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can see that we achieve similar results to the first example although with less weights and faster training \n",
    "time. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/\n",
    "- https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
