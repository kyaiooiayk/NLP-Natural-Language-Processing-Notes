{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What? Training embedding\n",
    "\n",
    "Word embeddings are an approach to representing text in NLP. In this notebook we will demonstrate how to train \n",
    "embeddings using Genism\n",
    "\n",
    "Reference: https://github.com/practical-nlp/practical-nlp/blob/master/Ch3/06_Training_embeddings_using_gensim.ipynb\n",
    "           Harshit Surana, Practical Natural Language Processing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import requests\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:40.863650Z",
     "start_time": "2021-04-05T21:26:40.339123Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TBw9OCYcYQ_n"
   },
   "source": [
    "# Define training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Genism word2vec requires that a format of ‘list of lists’ be provided for training where every document \n",
    "contained in a list. Every list contains lists of tokens of that document.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dog', 'bites', 'man'],\n",
       " ['man', 'bites', 'dog'],\n",
       " ['dog', 'eats', 'meat'],\n",
       " ['man', 'eats', 'food']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [['dog','bites','man'], [\"man\", \"bites\" ,\"dog\"],[\"dog\",\"eats\",\"meat\"],[\"man\", \"eats\",\"food\"]]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Two different learning models were introduced as part of the word2vec approach to learn the word embedding; \n",
    "they are:\n",
    "    [1] Continuous Bag-of-Words, or CBOW model.\n",
    "    [2] Continuous Skip-Gram Model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:40.894143Z",
     "start_time": "2021-04-05T21:26:40.865114Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5qWptd54ZcfV"
   },
   "outputs": [],
   "source": [
    "# Using CBOW Architecture for trainnig\n",
    "model_cbow     = Word2Vec(corpus, min_count=1,sg=0) \n",
    "# Using skipGram Architecture for training \n",
    "model_skipgram = Word2Vec(corpus, min_count=1,sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QjSxefPl4mh"
   },
   "source": [
    "# Continuous Bag of Words (CBOW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In CBOW, the primary task is to build a language model that correctly predicts the center word given the context \n",
    "words in which the center word appears.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:56.724662Z",
     "start_time": "2021-04-05T21:26:56.712651Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "nyZY8ME4lUjd",
    "outputId": "bd00e825-c11a-4b36-dbf5-80f32c659956",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, vector_size=100, alpha=0.025)\n",
      "['man', 'dog', 'eats', 'bites', 'food', 'meat']\n",
      "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419371e-03\n",
      "  7.4669169e-03 -6.1676763e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400517e-03 -6.1735227e-03 -4.1022300e-04 -8.3689503e-03\n",
      " -5.6000138e-03  7.1045374e-03  3.3525396e-03  7.2256685e-03\n",
      "  6.8002464e-03  7.5307419e-03 -3.7891555e-03 -5.6180713e-04\n",
      "  2.3483753e-03 -4.5190332e-03  8.3887316e-03 -9.8581649e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328329e-03  4.3981862e-03\n",
      " -1.7395759e-03  6.7113829e-03  9.9648498e-03 -4.3624449e-03\n",
      " -5.9933902e-04 -5.6956387e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384959e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895051e-03 -9.1558648e-03 -3.5575390e-04\n",
      " -3.0998420e-03  7.8943158e-03  5.9385728e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900396e-03  7.8175711e-03 -9.5101884e-03\n",
      " -2.0553112e-04  3.4691954e-03 -9.3897345e-04  8.3817719e-03\n",
      "  9.0107825e-03  6.5365052e-03 -7.1162224e-04  7.7104042e-03\n",
      " -8.5343365e-03  3.2071066e-03 -4.6379971e-03 -5.0889566e-03\n",
      "  3.5896183e-03  5.3703380e-03  7.7695129e-03 -5.7665063e-03\n",
      "  7.4333595e-03  6.6254949e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097548e-03 -7.8755140e-04 -6.7098569e-03\n",
      " -7.0859264e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748782e-03  2.4402141e-05 -9.8835090e-03\n",
      "  2.6920033e-03 -4.7501065e-03  1.0876465e-03 -1.5762257e-03\n",
      "  2.1966719e-03 -7.8815771e-03 -2.7171851e-03  2.6631975e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100952e-03  4.5058774e-03]\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(model_cbow)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(model_cbow.wv.index_to_key)\n",
    "print(words)\n",
    "\n",
    "# Access vector for one word\n",
    "print(model_cbow.wv['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:57.420196Z",
     "start_time": "2021-04-05T21:26:57.417193Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "gMuHv52GeuoR",
    "outputId": "b498032d-6f9d-485b-a3cc-5a21300bfb06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between eats and bites: -0.013497097\n",
      "Similarity between eats and man: -0.052354384\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity \n",
    "print(\"Similarity between eats and bites:\",model_cbow.wv.similarity('eats', 'bites'))\n",
    "print(\"Similarity between eats and man:\",model_cbow.wv.similarity('eats', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "twhTZfPOezTU"
   },
   "source": [
    "From the above similarity scores we can conclude that eats is more similar to bites than man."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:59.635831Z",
     "start_time": "2021-04-05T21:26:59.621818Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "5Lv0V7WofmsB",
    "outputId": "00600b23-d9a6-4f14-bacd-395be85076c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 0.13887985050678253),\n",
       " ('bites', 0.13149003684520721),\n",
       " ('eats', 0.06422408670186996),\n",
       " ('dog', 0.009391186758875847),\n",
       " ('man', -0.05987628176808357)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most similarity\n",
    "model_cbow.wv.most_similar('meat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:26:59.855822Z",
     "start_time": "2021-04-05T21:26:59.841810Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WA783nrSalgs",
    "outputId": "80d6e23f-2bed-47d7-f925-4aaa87ec5f9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_cbow.save('model_cbow.bin')\n",
    "\n",
    "# load model\n",
    "new_model_cbow = Word2Vec.load('model_cbow.bin')\n",
    "print(new_model_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "deReLSI7mQyr"
   },
   "source": [
    "# SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In skipgram, the task is to predict the context words from the center word.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:27:00.517046Z",
     "start_time": "2021-04-05T21:27:00.508038Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "9QtUtsLglvY0",
    "outputId": "6d19902b-66aa-4b0f-9f12-be18f37d40d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, vector_size=100, alpha=0.025)\n",
      "['man', 'dog', 'eats', 'bites', 'food', 'meat']\n",
      "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419371e-03\n",
      "  7.4669169e-03 -6.1676763e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400517e-03 -6.1735227e-03 -4.1022300e-04 -8.3689503e-03\n",
      " -5.6000138e-03  7.1045374e-03  3.3525396e-03  7.2256685e-03\n",
      "  6.8002464e-03  7.5307419e-03 -3.7891555e-03 -5.6180713e-04\n",
      "  2.3483753e-03 -4.5190332e-03  8.3887316e-03 -9.8581649e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328329e-03  4.3981862e-03\n",
      " -1.7395759e-03  6.7113829e-03  9.9648498e-03 -4.3624449e-03\n",
      " -5.9933902e-04 -5.6956387e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384959e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895051e-03 -9.1558648e-03 -3.5575390e-04\n",
      " -3.0998420e-03  7.8943158e-03  5.9385728e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900396e-03  7.8175711e-03 -9.5101884e-03\n",
      " -2.0553112e-04  3.4691954e-03 -9.3897345e-04  8.3817719e-03\n",
      "  9.0107825e-03  6.5365052e-03 -7.1162224e-04  7.7104042e-03\n",
      " -8.5343365e-03  3.2071066e-03 -4.6379971e-03 -5.0889566e-03\n",
      "  3.5896183e-03  5.3703380e-03  7.7695129e-03 -5.7665063e-03\n",
      "  7.4333595e-03  6.6254949e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097548e-03 -7.8755140e-04 -6.7098569e-03\n",
      " -7.0859264e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748782e-03  2.4402141e-05 -9.8835090e-03\n",
      "  2.6920033e-03 -4.7501065e-03  1.0876465e-03 -1.5762257e-03\n",
      "  2.1966719e-03 -7.8815771e-03 -2.7171851e-03  2.6631975e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100952e-03  4.5058774e-03]\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(model_skipgram)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(model_skipgram.wv.index_to_key)\n",
    "print(words)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(model_skipgram.wv['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:27:02.660747Z",
     "start_time": "2021-04-05T21:27:02.642866Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "8YUsblEOfFWf",
    "outputId": "14cd759c-d5fc-465f-ed20-8fd1a1949168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between eats and bites: -0.01351881\n",
      "Similarity between eats and man: -0.05234512\n"
     ]
    }
   ],
   "source": [
    "#Compute similarity \n",
    "print(\"Similarity between eats and bites:\",model_skipgram.wv.similarity('eats', 'bites'))\n",
    "print(\"Similarity between eats and man:\",model_skipgram.wv.similarity('eats', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdXVDePKnBpv"
   },
   "source": [
    "From the above similarity scores we can conclude that eats is more similar to bites than man."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:27:03.419546Z",
     "start_time": "2021-04-05T21:27:03.414541Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "lpF4qtwpmuM3",
    "outputId": "f3bc68f6-3768-4a4d-e5bc-bb3dff6f654f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 0.13887986540794373),\n",
       " ('bites', 0.1314900517463684),\n",
       " ('eats', 0.06406084448099136),\n",
       " ('dog', 0.009391188621520996),\n",
       " ('man', -0.059876274317502975)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most similarity\n",
    "model_skipgram.wv.most_similar('meat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:27:03.973454Z",
     "start_time": "2021-04-05T21:27:03.950433Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aNDCEXRTnAnj",
    "outputId": "402f77b6-0625-4b37-e135-3650df626007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_skipgram.save('model_skipgram.bin')\n",
    "\n",
    "# load model\n",
    "new_model_skipgram = Word2Vec.load('model_skipgram.bin')\n",
    "print(model_skipgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Your Embedding on Wiki Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "b0MiqJ_1M0mX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The corpus download page : https://dumps.wikimedia.org/enwiki/20200120/\n",
    "The entire wiki corpus as of 28/04/2020 is just over 16GB in size.\n",
    "We will take a part of this corpus due to computation constraints and train our word2vec and fasttext embeddings.\n",
    "\n",
    "The file size is 294MB so it can take a while to download.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T21:27:58.596845Z",
     "start_time": "2021-04-05T21:27:58.585833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File at: data/en/enwiki-latest-pages-articles-multistream14.xml-p13159683p14324602.bz2\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('data/en', exist_ok= True)\n",
    "file_name = \"data/en/enwiki-latest-pages-articles-multistream14.xml-p13159683p14324602.bz2\"\n",
    "file_id = \"11804g0GcWnBIVDahjo5fQyc05nQLXGwF\"\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    download_file_from_google_drive(file_id, file_name)\n",
    "else:\n",
    "    print(\"file already exists, skipping download\")\n",
    "\n",
    "print(f\"File at: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T09:56:14.722195Z",
     "start_time": "2021-04-03T09:56:14.705177Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rJgsEUmRPppc"
   },
   "outputs": [],
   "source": [
    "#Preparing the Training data\n",
    "wiki = WikiCorpus(file_name, dictionary={})\n",
    "sentences = list(wiki.get_texts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "xsIrgt_gPQda"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.   sg - Selecting the training algorithm: 1 for skip-gram else its 0 for CBOW. Default is CBOW.\n",
    "2.   min_count-  Ignores all words with total frequency lower than this.<br>\n",
    "There are many more hyperparamaeters whose list can be found in the official documentation [here.]\n",
    "(https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:01:20.065332Z",
     "start_time": "2021-04-03T09:59:12.350872Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "idmfbr_8LvoN",
    "outputId": "f505a46e-025d-4169-f996-06c672008f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Model Training Complete.\n",
      "Time taken for training is:0.05 hrs \n"
     ]
    }
   ],
   "source": [
    "#CBOW\n",
    "start = time.time()\n",
    "word2vec_cbow = Word2Vec(sentences,min_count=10, sg=0)\n",
    "end = time.time()\n",
    "\n",
    "print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:02:10.613551Z",
     "start_time": "2021-04-03T10:02:10.585535Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "mMdGn08-RkhM",
    "outputId": "efb34148-3fb4-435c-f070-8493708fc07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=111150, vector_size=100, alpha=0.025)\n",
      "------------------------------\n",
      "Length of vocabulary: 111150\n",
      "Printing the first 30 words.\n",
      "['the', 'of', 'and', 'in', 'to', 'was', 'is', 'for', 'on', 'as', 'by', 'with', 'he', 'at', 'from', 'that', 'his', 'it', 'an', 'also', 'which', 'were', 'are', 'this', 'new', 'first', 'be', 'or', 'one', 'has']\n",
      "------------------------------\n",
      "Length of vector: 100\n",
      "[-1.7285163e+00 -6.6549134e-01 -5.5782366e-01 -1.7490451e+00\n",
      " -2.3600571e+00  2.7023544e+00 -2.8793383e+00  3.3324206e+00\n",
      "  2.5752574e-01  9.2314738e-01 -2.4496200e+00 -9.8786438e-01\n",
      "  2.8701024e+00 -1.5976252e-02  2.1427767e+00 -3.1982270e-01\n",
      " -1.9374223e-01  1.5745129e-04  3.6849504e+00  8.4056181e-01\n",
      "  1.0534444e+00  1.2753040e+00 -2.7505445e-01  2.6600546e-01\n",
      " -1.1139123e+00  3.2871590e+00 -1.7120357e+00 -3.5111151e+00\n",
      "  1.7522961e-01  1.5176815e+00 -6.6908938e-01 -2.5252836e+00\n",
      " -3.8272228e-02  2.5019100e+00  1.9924344e+00  6.7706496e-01\n",
      "  1.3728181e-01  4.7461057e-01 -5.0196785e-01 -1.2713498e+00\n",
      " -7.2165263e-01 -3.2813940e+00 -1.4064248e+00  1.3198926e+00\n",
      " -2.0050631e+00  8.4949392e-01 -1.0452318e+00  4.0827188e-01\n",
      "  2.4470525e+00  3.4450829e-01  1.9690307e+00  4.1206902e-01\n",
      " -2.0920258e+00  2.3748057e+00 -4.1456336e-01 -1.0031059e+00\n",
      " -3.1958060e+00 -2.0121231e+00 -1.6689576e-01  1.9314526e+00\n",
      " -2.1203502e-01 -1.7849292e+00  5.9669709e-01  1.5484785e+00\n",
      "  2.3549898e+00 -4.6931067e-01 -2.5419786e+00  4.5307231e+00\n",
      " -2.6822469e-01 -1.6341003e+00 -7.7153707e-01  2.5762916e-01\n",
      " -1.1228235e+00  1.3941602e+00  2.3036754e+00 -8.1363118e-01\n",
      "  4.7281866e+00  1.2249984e+00  1.4225631e+00 -1.4933816e-01\n",
      "  1.5446904e+00 -4.0128967e-01  9.6577066e-01 -2.7916124e+00\n",
      "  2.9873643e+00 -8.9174527e-01 -3.2371652e+00 -8.0476427e-01\n",
      " -9.3619269e-01 -2.9729009e+00  1.6358142e+00 -1.0151066e+00\n",
      "  9.7738314e-01  4.0230303e+00  1.6849039e+00 -9.3209511e-01\n",
      " -2.2211323e+00  3.4523666e+00  1.1957079e+00 -2.8263848e+00]\n",
      "------------------------------\n",
      "Similarity between film and drama: 0.5250465\n",
      "Similarity between film and tiger: 0.16909045\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(word2vec_cbow)\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(word2vec_cbow.wv.index_to_key)\n",
    "print(f\"Length of vocabulary: {len(words)}\")\n",
    "print(\"Printing the first 30 words.\")\n",
    "print(words[:30])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(f\"Length of vector: {len(word2vec_cbow.wv['film'])}\")\n",
    "print(word2vec_cbow.wv['film'])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Compute similarity \n",
    "print(\"Similarity between film and drama:\",word2vec_cbow.wv.similarity('film', 'drama'))\n",
    "print(\"Similarity between film and tiger:\",word2vec_cbow.wv.similarity('film', 'tiger'))\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:02:16.109851Z",
     "start_time": "2021-04-03T10:02:15.257052Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rXrDOrKskcHX"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "from gensim.models import Word2Vec, KeyedVectors   \n",
    "word2vec_cbow.wv.save_word2vec_format('word2vec_cbow.bin', binary=True)\n",
    "\n",
    "# load model\n",
    "# new_modelword2vec_cbow = Word2Vec.load('word2vec_cbow.bin')\n",
    "# print(word2vec_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:08:27.736688Z",
     "start_time": "2021-04-03T10:02:19.197708Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "dX0U0CbQOK30",
    "outputId": "b9bfcf2b-91cb-40d9-ca92-791ec346aef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram Model Training Complete\n",
      "Time taken for training is:0.16 hrs \n"
     ]
    }
   ],
   "source": [
    "#SkipGram\n",
    "start = time.time()\n",
    "word2vec_skipgram = Word2Vec(sentences,min_count=10, sg=1)\n",
    "end = time.time()\n",
    "\n",
    "print(\"SkipGram Model Training Complete\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:09:06.406929Z",
     "start_time": "2021-04-03T10:09:06.383908Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "LXnY9YInSvnI",
    "outputId": "26f1dab7-27a6-4655-81c7-ac6f08fe1f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=111150, vector_size=100, alpha=0.025)\n",
      "------------------------------\n",
      "Length of vocabulary: 111150\n",
      "Printing the first 30 words.\n",
      "['the', 'of', 'and', 'in', 'to', 'was', 'is', 'for', 'on', 'as', 'by', 'with', 'he', 'at', 'from', 'that', 'his', 'it', 'an', 'also', 'which', 'were', 'are', 'this', 'new', 'first', 'be', 'or', 'one', 'has']\n",
      "------------------------------\n",
      "Length of vector: 100\n",
      "[-0.22242647 -0.33629146 -0.09735668  0.3212172   0.20769584 -0.4093927\n",
      "  0.2601584   0.4353261   0.05722366 -0.04553508 -0.2456233  -0.825595\n",
      "  0.49114382  0.37693536  0.32347617  0.08100371  0.50564265  0.4605116\n",
      " -0.6479015  -0.22675617  0.30894187 -0.37535283 -0.23129302 -0.41158074\n",
      " -0.55922776 -0.00926834  0.13279851 -0.0278654   0.00178477  0.31258506\n",
      " -0.07889551 -0.6536966   0.3207598   0.01975377  0.06030234  0.45609853\n",
      "  0.09996798  0.23685017 -0.6418596   0.09482809 -0.20822836 -0.2828217\n",
      " -0.37507758  0.2892338  -0.22006956  0.35175303 -0.40061903  0.18101247\n",
      "  0.12281884  0.07366457 -0.10642077 -0.26944295 -0.04773464 -0.18893798\n",
      "  0.28226507  0.42097333  0.4112865   0.02672396  0.02317124  0.23626108\n",
      "  0.04905589 -0.3325342   0.05400299  0.01184283 -0.02488856 -0.16424803\n",
      "  0.03153074  0.81727886 -0.42712003 -0.40304655 -0.4799002   0.45654142\n",
      "  0.02228257 -0.61552775  0.28495023  0.26295802  0.67173284  0.18484735\n",
      " -0.11916874  0.10061626 -0.22107358  0.50540596 -0.83707273 -0.09850102\n",
      "  0.20993462 -0.30754453  0.00189317 -0.31111974  0.07891352 -0.4308658\n",
      "  0.00816268 -0.4420314   0.01626863  0.5316948  -0.00279754  0.10899547\n",
      "  0.22793053 -0.3854801   0.6325212   0.04136872]\n",
      "------------------------------\n",
      "Similarity between film and drama: 0.6473753\n",
      "Similarity between film and tiger: 0.25493917\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(word2vec_skipgram)\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(word2vec_skipgram.wv.index_to_key)\n",
    "print(f\"Length of vocabulary: {len(words)}\")\n",
    "print(\"Printing the first 30 words.\")\n",
    "print(words[:30])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(f\"Length of vector: {len(word2vec_skipgram.wv['film'])}\")\n",
    "print(word2vec_skipgram.wv['film'])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Compute similarity \n",
    "print(\"Similarity between film and drama:\",word2vec_skipgram.wv.similarity('film', 'drama'))\n",
    "print(\"Similarity between film and tiger:\",word2vec_skipgram.wv.similarity('film', 'tiger'))\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:09:09.947695Z",
     "start_time": "2021-04-03T10:09:09.076901Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "o8U7bfPSVB04"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "word2vec_cbow.wv.save_word2vec_format('word2vec_sg.bin', binary=True)\n",
    "\n",
    "# load model\n",
    "# new_model_skipgram = Word2Vec.load('model_skipgram.bin')\n",
    "# print(model_skipgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kExlA8kfrKml"
   },
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When we have a large dataset, and when learning seems infeasible with the approaches described so far, \n",
    "fastText is a good option to use to set up a strong working baseline. However, there’s one concern to keep \n",
    "in mind when using fastText, as was the case with Word2vec embeddings: it uses pre-trained character n-gram\n",
    "embeddings. Thus, when we save the trained model, it carries the entire character n-gram embeddings dictionary \n",
    "with it. This results in a bulky model and can result in engineering issues. For example, the model stored with \n",
    "the name “temp” in the above code snippet has a size close to 450 MB. However, fastText implementation also comes\n",
    "with options to reduce the memory footprint of its classification models with minimal reduction in classification\n",
    "performance.\n",
    "\n",
    "\n",
    "Some of the most popular pre-trained embeddings are\n",
    "    [1] Word2vec by Google\n",
    "    [2] GloVe    by Stanford\n",
    "    [3] fastText by Facebook\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:16:31.271764Z",
     "start_time": "2021-04-03T10:09:16.592670Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "JPd2VhMEk8gL",
    "outputId": "55c44bdd-d7d8-4df2-8140-cdd442bbd68c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText CBOW Model Training Complete\n",
      "Time taken for training is:0.18 hrs \n"
     ]
    }
   ],
   "source": [
    "#CBOW\n",
    "start = time.time()\n",
    "fasttext_cbow = FastText(sentences, sg=0, min_count=10)\n",
    "end = time.time()\n",
    "\n",
    "print(\"FastText CBOW Model Training Complete\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:16:31.287283Z",
     "start_time": "2021-04-03T10:16:31.273765Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "FlQFl8-Zsost",
    "outputId": "6472e944-e6de-4d64-8c6f-14475ef1eac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=111150, vector_size=100, alpha=0.025)\n",
      "------------------------------\n",
      "Length of vocabulary: 111150\n",
      "Printing the first 30 words.\n",
      "['the', 'of', 'and', 'in', 'to', 'was', 'is', 'for', 'on', 'as', 'by', 'with', 'he', 'at', 'from', 'that', 'his', 'it', 'an', 'also', 'which', 'were', 'are', 'this', 'new', 'first', 'be', 'or', 'one', 'has']\n",
      "------------------------------\n",
      "Length of vector: 100\n",
      "[-4.1463914  -1.8119481   1.6104423   1.7290591  -2.0994642  -0.631982\n",
      " -2.0095596   2.063677    4.921095    1.2384105   3.405752    1.2418664\n",
      " -5.2132835   1.425097    0.7388006   0.83315045  1.4783679  -1.4807229\n",
      "  2.7253404   5.286211   -1.8287735   4.7407475  -2.7341452   2.166024\n",
      " -0.16232926  0.84643435  2.6543214   0.5374928   1.3685168   1.106021\n",
      " -0.25491908 -0.82786995  0.4071864  -2.0010269  -2.6835878   2.781833\n",
      "  4.4211655   3.3429868   5.1445546   0.14225101 -4.0890293  -4.4640427\n",
      "  0.13031602  0.44952396 -3.9834783   0.645766    3.8364275   4.564231\n",
      " -0.17106022  2.961449    2.06758     0.4656901  -4.8109646   5.868913\n",
      "  0.4649309  -2.3303418   1.4615048   1.757864    2.7056413   0.37692165\n",
      "  3.299511   -1.3021749  -3.155448    1.9806092   6.2725863   3.2077596\n",
      " -0.3041754  -3.562989    0.9858324  -0.17864844 -2.379224    5.6213884\n",
      "  2.5834236  -1.6374314  -4.2799764   1.8195857   3.6953878   3.2594297\n",
      "  0.36243895 -1.0769662   3.3386335  -1.6935852  -2.5365698   2.6877594\n",
      "  0.85165256 -4.9921846  -1.6755143  -4.4908175  -5.166348    1.3659352\n",
      "  5.879543    1.1466843  -3.547712    4.5901966   1.3708478  -3.5130844\n",
      " -5.576376    1.1377964  -1.2097398   0.6872596 ]\n",
      "------------------------------\n",
      "Similarity between film and drama: 0.60324234\n",
      "Similarity between film and tiger: 0.22874333\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(fasttext_cbow)\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(fasttext_cbow.wv.index_to_key)\n",
    "print(f\"Length of vocabulary: {len(words)}\")\n",
    "print(\"Printing the first 30 words.\")\n",
    "print(words[:30])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(f\"Length of vector: {len(fasttext_cbow.wv['film'])}\")\n",
    "print(fasttext_cbow.wv['film'])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Compute similarity \n",
    "print(\"Similarity between film and drama:\",fasttext_cbow.wv.similarity('film', 'drama'))\n",
    "print(\"Similarity between film and tiger:\",fasttext_cbow.wv.similarity('film', 'tiger'))\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:28:28.771383Z",
     "start_time": "2021-04-03T10:16:31.289284Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "UgSOxsNklAvh",
    "outputId": "f491f83c-17b8-42ad-a225-479df8419578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText SkipGram Model Training Complete\n",
      "Time taken for training is:0.29 hrs \n"
     ]
    }
   ],
   "source": [
    "#SkipGram\n",
    "start = time.time()\n",
    "fasttext_skipgram = FastText(sentences, sg=1, min_count=10)\n",
    "end = time.time()\n",
    "\n",
    "print(\"FastText SkipGram Model Training Complete\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T10:28:28.803412Z",
     "start_time": "2021-04-03T10:28:28.773386Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "vFiTAP0PsQwi",
    "outputId": "a29ae2e3-5dbc-453a-f66b-ceca255a8652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=111150, vector_size=100, alpha=0.025)\n",
      "------------------------------\n",
      "Length of vocabulary: 111150\n",
      "Printing the first 30 words.\n",
      "['the', 'of', 'and', 'in', 'to', 'was', 'is', 'for', 'on', 'as', 'by', 'with', 'he', 'at', 'from', 'that', 'his', 'it', 'an', 'also', 'which', 'were', 'are', 'this', 'new', 'first', 'be', 'or', 'one', 'has']\n",
      "------------------------------\n",
      "Length of vector: 100\n",
      "[ 0.30401322  0.3009671  -0.5875729   0.27169418  0.47659576  0.38756916\n",
      "  0.09156659 -0.15275064  0.57474303 -0.07595404 -0.13621216  0.25278902\n",
      " -0.34578922  1.0397061  -0.5496711  -0.3047049  -0.02724782  0.44049594\n",
      " -0.05332035  0.02233534  0.09534271  0.44290674  0.39798424  0.3414503\n",
      " -0.2835456   0.09747949 -0.2093019   0.05018875  0.40034446  0.05758088\n",
      " -0.33358884  0.14034662 -0.0479691   0.4520111  -0.24378438 -0.39850605\n",
      "  0.24458605  0.37684715  0.00532099  0.12791194  0.44664317 -0.20575352\n",
      " -0.06274722  0.2542009   0.03270878  0.2436347  -0.6695596   0.17473593\n",
      "  0.17213994  0.23949255  0.46619684  0.30722624 -0.1237082   0.03643462\n",
      " -0.23632497  0.06980287  0.01852006  0.18564536  0.6566505  -0.1530746\n",
      "  0.0021976   0.16895814  0.28658667  0.39515164 -0.00842195  0.6382573\n",
      " -0.31671342 -0.5761363   0.11916251 -0.14351939 -0.29520532  0.56398666\n",
      "  0.46659938 -0.00465837  0.0045834  -0.01961587  0.23824215  0.46098617\n",
      " -0.31329906 -0.36071816  0.17940521 -0.08908519  0.09946502  0.04348784\n",
      " -0.27785718 -0.6545029   0.379305   -0.864645   -0.1415615   0.41304588\n",
      "  0.46643952  0.00661104  0.00470975  0.2808194  -0.00252593  0.15334864\n",
      "  0.43310058  0.45334566 -0.32470557  0.05097945]\n",
      "------------------------------\n",
      "Similarity between film and drama: 0.6223798\n",
      "Similarity between film and tiger: 0.35100698\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(fasttext_skipgram)\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(fasttext_skipgram.wv.index_to_key)\n",
    "print(f\"Length of vocabulary: {len(words)}\")\n",
    "print(\"Printing the first 30 words.\")\n",
    "print(words[:30])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(f\"Length of vector: {len(fasttext_skipgram.wv['film'])}\")\n",
    "print(fasttext_skipgram.wv['film'])\n",
    "print(\"-\"*30)\n",
    "\n",
    "#Compute similarity \n",
    "print(\"Similarity between film and drama:\",fasttext_skipgram.wv.similarity('film', 'drama'))\n",
    "print(\"Similarity between film and tiger:\",fasttext_skipgram.wv.similarity('film', 'tiger'))\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "oArMIJzYOmUR"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An interesting obeseravtion if you noticed is that CBOW trains faster than SkipGram in both cases.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "Training_embeddings_using_gensim.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
