{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Finetuning-Pre-trained-BERT-Model-on-Text-Classification-Task-And-Inferencing-with-ONNX-Runtime\" data-toc-modified-id=\"Finetuning-Pre-trained-BERT-Model-on-Text-Classification-Task-And-Inferencing-with-ONNX-Runtime-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Finetuning Pre-trained BERT Model on Text Classification Task And Inferencing with ONNX Runtime</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tokenizer\" data-toc-modified-id=\"Tokenizer-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Tokenizer</a></span></li><li><span><a href=\"#Model-FineTuning\" data-toc-modified-id=\"Model-FineTuning-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Model FineTuning</a></span></li></ul></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:22:54.474633Z",
     "start_time": "2023-01-04T15:22:54.452949Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Pre-trained BERT Model on Text Classification Task And Inferencing with ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, we'll be going over two main things:\n",
    "\n",
    "- Process of finetuning a pre-trained BERT model towards a text classification task, more specificially, the [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs/data) challenge.\n",
    "- Process of converting our model into [ONNX](https://onnx.ai/) format, and perform inferencing benchmark with [ONNX runtime](https://www.onnxruntime.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:24:58.713800Z",
     "start_time": "2023-01-04T15:24:58.687048Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportWarning",
     "evalue": "To use `datasets`, the module `pyarrow>=6.0.0` is required, and the current version of `pyarrow` doesn't match this condition.\nIf you are running this in a Google Colab, you should probably just restart the runtime to use the right version of `pyarrow`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportWarning\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5x/lyqr8wv507n43bnwpsz4dq_c0000gn/T/ipykernel_59193/3186227788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quora\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/trainingAI/lib/python3.9/site-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     raise ImportWarning(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;34m\"To use `datasets`, the module `pyarrow>=6.0.0` is required, and the current version of `pyarrow` doesn't match this condition.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;34m\"If you are running this in a Google Colab, you should probably just restart the runtime to use the right version of `pyarrow`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportWarning\u001b[0m: To use `datasets`, the module `pyarrow>=6.0.0` is required, and the current version of `pyarrow` doesn't match this condition.\nIf you are running this in a Google Colab, you should probably just restart the runtime to use the right version of `pyarrow`."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "dataset_dict = load_dataset(\"quora\")\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.082809Z",
     "start_time": "2023-01-04T15:21:42.082797Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.083787Z",
     "start_time": "2023-01-04T15:21:42.083774Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "val_size = 0.1\n",
    "dataset_dict_test = dataset_dict['train'].train_test_split(test_size=test_size)\n",
    "dataset_dict_train_val = dataset_dict_test['train'].train_test_split(test_size=val_size)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset_dict_train_val[\"train\"],\n",
    "    \"val\": dataset_dict_train_val[\"test\"],\n",
    "    \"test\": dataset_dict_test[\"test\"]\n",
    "})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be going over the details of the pre-trained tokenizer or model and only load a pre-trained one available from the huggingface model repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:25:18.259253Z",
     "start_time": "2023-01-04T15:25:13.071851Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slow version of gensim.models.doc2vec is being used\n",
      "Slow version of Fasttext is being used\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoTokenizer' from 'transformers' (/Users/gm_main/Desktop/DataScienceFolder/GitHub/NLP-Natural-Language-Processing-Notes/tutorials/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5x/lyqr8wv507n43bnwpsz4dq_c0000gn/T/ipykernel_59193/3070641033.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# https://huggingface.co/transformers/model_doc/distilbert.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"distilbert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoTokenizer' from 'transformers' (/Users/gm_main/Desktop/DataScienceFolder/GitHub/NLP-Natural-Language-Processing-Notes/tutorials/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/transformers/model_doc/distilbert.html\n",
    "pretrained_model_name_or_path = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed our tokenizer directly with a pair of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.086032Z",
     "start_time": "2023-01-04T15:21:42.086020Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\n",
    "    'What is the step by step guide to invest in share market in india?',\n",
    "    'What is the step by step guide to invest in share market?'\n",
    ")\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding the tokenized inputs, this model's tokenizer adds some special tokens such as, `[SEP]`, that is used to indicate which token belongs to which segment/pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.087154Z",
     "start_time": "2023-01-04T15:21:42.087140Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proprocessing step will be task specific, if we happen to be using another dataset, this function needs to be modified accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.088378Z",
     "start_time": "2023-01-04T15:21:42.088365Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    labels = [int(label) for label in examples['is_duplicate']]\n",
    "    texts = [question['text'] for question in examples['questions']]\n",
    "    texts1 = [text[0] for text in texts]\n",
    "    texts2 = [text[1] for text in texts]\n",
    "    tokenized_examples = tokenizer(texts1, texts2)\n",
    "    tokenized_examples['labels'] = labels\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.089851Z",
     "start_time": "2023-01-04T15:21:42.089834Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dict_tokenized = dataset_dict.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=['is_duplicate', 'questions']\n",
    ")\n",
    "dataset_dict_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.091436Z",
     "start_time": "2023-01-04T15:21:42.091418Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dict_tokenized['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model FineTuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having preprocessed our raw dataset, for our text classification task, we use `AutoModelForSequenceClassification` class to load the pre-trained model, the only other argument we need to specify is the number of class/label our text classification task has. Upon instantiating the model for the first time, we'll see some warnings generated, telling us we should fine tune this model on our down stream tasks before using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.093360Z",
     "start_time": "2023-01-04T15:21:42.093343Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'text_classification'\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.094806Z",
     "start_time": "2023-01-04T15:21:42.094790Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# we'll save the model after fine tuning it once, so we can skip the fine tuning part during\n",
    "# the second round if we detect that we already have one available\n",
    "if os.path.isdir(model_checkpoint):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path, num_labels=num_labels)\n",
    "\n",
    "print('# of parameters: ', model.num_parameters())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.096090Z",
     "start_time": "2023-01-04T15:21:42.096073Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform all sorts of hyper parameter tuning on the fine tuning step, here we'll pick some default parameters for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.097232Z",
     "start_time": "2023-01-04T15:21:42.097218Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "args = TrainingArguments(\n",
    "    \"quora\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_dict_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_dict_tokenized['val']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.098607Z",
     "start_time": "2023-01-04T15:21:42.098593Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(model_checkpoint):\n",
    "    trainer.train()\n",
    "    model.save_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next couple of code chunks performs batch inferencing on our dataset, and reports standard binary classification evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.100002Z",
     "start_time": "2023-01-04T15:21:42.099986Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict(model, example, round_digits: int = 5):\n",
    "    input_ids = example['input_ids'].to(model.device)\n",
    "    attention_mask = example['attention_mask'].to(model.device)\n",
    "    batch_labels = example['labels'].detach().cpu().numpy().tolist()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_output = model(input_ids, attention_mask)\n",
    "\n",
    "    batch_scores = F.softmax(batch_output.logits, dim=-1)[:, 1]\n",
    "    batch_scores = np.round(batch_scores.detach().cpu().numpy(), round_digits).tolist()\n",
    "    return batch_scores, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.101216Z",
     "start_time": "2023-01-04T15:21:42.101200Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def predict_data_loader(model, data_loader: DataLoader) -> pd.DataFrame:\n",
    "    scores = []\n",
    "    labels = []\n",
    "    for example in data_loader:\n",
    "        batch_scores, batch_labels = predict(model, example)\n",
    "        scores += batch_scores\n",
    "        labels += batch_labels\n",
    "\n",
    "    df_predictions = pd.DataFrame.from_dict({'scores': scores, 'labels': labels})\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.102474Z",
     "start_time": "2023-01-04T15:21:42.102461Z"
    }
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset_dict_tokenized['test'], collate_fn=data_collator, batch_size=64)\n",
    "start = time.time()\n",
    "df_predictions = predict_data_loader(model, data_loader)\n",
    "end = time.time()\n",
    "print('elapsed: ', end - start)\n",
    "print(df_predictions.shape)\n",
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.103552Z",
     "start_time": "2023-01-04T15:21:42.103541Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def compute_binary_classification_metrics(y_true, y_score, round_digits: int = 3):\n",
    "    auc = metrics.roc_auc_score(y_true, y_score)\n",
    "    log_loss = metrics.log_loss(y_true, y_score)\n",
    "\n",
    "    precision, recall, threshold = metrics.precision_recall_curve(y_true, y_score)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    mask = ~np.isnan(f1)\n",
    "    f1 = f1[mask]\n",
    "    precision = precision[mask]\n",
    "    recall = recall[mask]\n",
    "\n",
    "    best_index = np.argmax(f1)\n",
    "    precision = precision[best_index]\n",
    "    recall = recall[best_index]\n",
    "    f1 = f1[best_index]\n",
    "    return {\n",
    "        'auc': round(auc, round_digits),\n",
    "        'precision': round(precision, round_digits),\n",
    "        'recall': round(recall, round_digits),\n",
    "        'f1': round(f1, round_digits),\n",
    "        'log_loss': round(log_loss, round_digits)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-04T15:21:42.104757Z",
     "start_time": "2023-01-04T15:21:42.104745Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_binary_classification_metrics(df_predictions['labels'], df_predictions['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Jupyter Notebook: Fine-tuning a model on a text classification task](https://nbviewer.org/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)\n",
    "- [Blog: Faster and smaller quantized NLP with Hugging Face and ONNX Runtime](https://medium.com/microsoftazure/faster-and-smaller-quantized-nlp-with-hugging-face-and-onnx-runtime-ec5525473bb7)\n",
    "- [PyTorch Documentation: Dynamic Quantization](https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html)\n",
    "- [Finetuning Pre-trained BERT Model on Text Classification Task](https://github.com/ethen8181/machine-learning/blob/master/model_deployment/transformers/text_classification_onnxruntime.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
