{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<font color=black><br>\n",
    "\n",
    "**What?** Bag of words, n-grams, TF-IDF\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "stopwordsNLTK = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- High level view of the method.\n",
    "\n",
    "- **STOPWORDS** are words which do not contain enough significance to be used without our algorithm. We would not \n",
    "want these words taking up space in our database, or taking up valuable processing time. For this, we can \n",
    "remove them easily by storing a list of words that you consider to be stop words.\n",
    "\n",
    "- **TOKENIZATION** is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, \n",
    "symbols and other elements called tokens. Tokens can be individual words, phrases or even whole sentences. \n",
    "In the process of tokenization, some characters like punctuation marks are discarded. \n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sentences in the corpus,in our case, just two \n",
    "string1 = \"Welcome to Great Learning , Now start learning\"\n",
    "string2 = \"Learning is a good practice\"\n",
    "string3 =  \"This is a good job. I will not miss it for anything\"\n",
    "string4 = \"This is not good at all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home made implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
      "['learning', 'is', 'a', 'good', 'practice']\n",
      "vocabulary ['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
      "Filterede vocabulary ['welcome', 'great', 'learning', 'start', 'good', 'practice']\n",
      "vectorise version [1, 1, 2, 1, 0, 0]\n",
      "vectorise version [0, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "\n",
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains \n",
    "    same and no item repeats. Using the set() function does not \n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "#print(stopwordsNLTK)\n",
    "stopwords= stopwordsNLTK #[\"to\",\"is\",\"a\"]\n",
    "\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "\n",
    "#convert them to lower case\n",
    "string1=string1.lower()\n",
    "string2=string2.lower()\n",
    "\n",
    "#split the sentences into tokens\n",
    "tokens1=string1.split()\n",
    "tokens2=string2.split()\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "\n",
    "#create a vocabulary list\n",
    "vocab=unique(tokens1+tokens2)\n",
    "print(\"vocabulary\", vocab)\n",
    "\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(\"Filterede vocabulary\", filtered_vocab)\n",
    "\n",
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "print(\"vectorise version\", vector1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(\"vectorise version\", vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nunigram ngram_range=(1,1)\\nbigrams ngram_range=(2,2)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "unigram ngram_range=(1,1)\n",
    "bigrams ngram_range=(2,2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>learning</th>\n",
       "      <th>practice</th>\n",
       "      <th>start</th>\n",
       "      <th>welcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good  great  learning  practice  start  welcome\n",
       "0     0      1         2         0      1        1\n",
       "1     1      0         1         1      0        0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVec = CountVectorizer(ngram_range=(1, 1), \n",
    "                           stop_words='english')\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform([string1, string2])\n",
    " \n",
    "#create dataframe\n",
    "cv_dataframe=pd.DataFrame(Count_data.toarray(), columns=CountVec.get_feature_names())\n",
    "cv_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- One hot encoding, BoW and TF-IDF treat words as independent units. \n",
    "- There is no notion of phrases or word ordering. \n",
    "- Bag of Ngrams (BoN) approach tries to remedy this. It does so by breaking text into chunks of n countigous words/tokens. \n",
    "- This can help us capture some context, which earlier approaches could not do. Let us see how it works using the same toy corpus we used in earlier examples.\n",
    "- Note that the number of features (and hence the size of the feature vector) increased a lot for the same data, compared to the ther single word based representations!! \n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>good practice</th>\n",
       "      <th>great</th>\n",
       "      <th>great learning</th>\n",
       "      <th>learning</th>\n",
       "      <th>learning good</th>\n",
       "      <th>learning start</th>\n",
       "      <th>practice</th>\n",
       "      <th>start</th>\n",
       "      <th>start learning</th>\n",
       "      <th>welcome</th>\n",
       "      <th>welcome great</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good  good practice  great  great learning  learning  learning good  \\\n",
       "0     0              0      1               1         2              0   \n",
       "1     1              1      0               0         1              1   \n",
       "\n",
       "   learning start  practice  start  start learning  welcome  welcome great  \n",
       "0               1         0      1               1        1              1  \n",
       "1               0         1      0               0        0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVec = CountVectorizer(ngram_range=(1, 2), \n",
    "                           stop_words='english')\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform([string1, string2])\n",
    " \n",
    "#create dataframe\n",
    "cv_dataframe=pd.DataFrame(Count_data.toarray(), columns=CountVec.get_feature_names())\n",
    "cv_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW with binary vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- In the above code, we represented the text considering the frequency of words into account. \n",
    "- However, sometimes, we don't care about frequency much, but only want to know whether a word appeared in a text or not. \n",
    "- That is, each document is represented as a vector of 0s and 1s. \n",
    "- We will use the option binary=True in CountVectorizer for this purpose.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>learning</th>\n",
       "      <th>practice</th>\n",
       "      <th>start</th>\n",
       "      <th>welcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good  great  learning  practice  start  welcome\n",
       "0     0      1         1         0      1        1\n",
       "1     1      0         1         1      0        0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVec = CountVectorizer(ngram_range=(1, 1), stop_words='english', binary=True)\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform([string1, string2])\n",
    " \n",
    "#create dataframe\n",
    "cv_dataframe=pd.DataFrame(Count_data.toarray(), columns=CountVec.get_feature_names())\n",
    "cv_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'good', 'job.', 'I', 'will', 'not', 'miss', 'it', 'for', 'anything']\n",
      "['This', 'is', 'not', 'good', 'at', 'all']\n",
      "vocabulary ['This', 'is', 'a', 'good', 'job.', 'I', 'will', 'not', 'miss', 'it', 'for', 'anything', 'at', 'all']\n",
      "Filterede vocabulary ['This', 'good', 'job.', 'I', 'miss', 'anything']\n",
      "vectorise version [1, 1, 1, 1, 1, 1]\n",
      "vectorise version [1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "#print(stopwordsNLTK)\n",
    "stopwords= stopwordsNLTK #[\"to\",\"is\",\"a\"]\n",
    "\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "\n",
    "#convert them to lower case\n",
    "string1=string3.lower()\n",
    "string2=string4.lower()\n",
    "\n",
    "#split the sentences into tokens\n",
    "tokens1=string3.split()\n",
    "tokens2=string4.split()\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "\n",
    "#create a vocabulary list\n",
    "vocab=unique(tokens1+tokens2)\n",
    "print(\"vocabulary\", vocab)\n",
    "\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(\"Filterede vocabulary\", filtered_vocab)\n",
    "\n",
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "print(\"vectorise version\", vector1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(\"vectorise version\", vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction with Tf-Idf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- Tf-Idf = term frequency-inverse document frequency\n",
    "- Note that the 1 added in the above formula is so that terms with zero IDF don’t get suppressed entirely.\n",
    "- This process is known as IDF smoothing.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Smoothing:\n",
      "       good       job      miss\n",
      "0  0.385372  0.652491  0.652491\n",
      "1  1.000000  0.000000  0.000000\n",
      "\n",
      "\n",
      "With Smoothing:\n",
      "       good       job      miss\n",
      "0  0.449436  0.631667  0.631667\n",
      "1  1.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "#without smooth IDF\n",
    "print(\"Without Smoothing:\")\n",
    "#define tf-idf\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=False,  \n",
    "                        ngram_range=(1,1),stop_words='english')\n",
    "#transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform([string3, string4])\n",
    " \n",
    "#create dataframe\n",
    "tf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
    "print(tf_idf_dataframe)\n",
    "print(\"\\n\")\n",
    " \n",
    "#with smooth\n",
    "tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \n",
    "                        smooth_idf=True,  \n",
    "                        ngram_range=(1,1),stop_words='english')\n",
    " \n",
    "tf_idf_data_smooth = tf_idf_vec_smooth.fit_transform([string3, string4])\n",
    " \n",
    "print(\"With Smoothing:\")\n",
    "tf_idf_dataframe_smooth=pd.DataFrame(tf_idf_data_smooth.toarray(),columns=tf_idf_vec_smooth.get_feature_names())\n",
    "print(tf_idf_dataframe_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=black><br>\n",
    "\n",
    "- In all the other approaches we saw so far, all the words in the text are treated equally important. \n",
    "- There is no notion of some words in the document being more important than others. \n",
    "- TF-IDF addresses this issue. \n",
    "- It aims to quantify the importance of a given word relative to other words in the document and in the corpus. \n",
    "- It was commonly used representation scheme for information retrieval systems, for extracting relevant documents from a corpus for given text query.\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words in the vocabulary ['bites', 'dog', 'eats', 'food', 'man', 'meat']\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names())\n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF representation for all documents in our corpus\n",
      " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
      " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# TFIDF representation for all documents in our corpus \n",
    "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf representation for 'dog and man are friends':\n",
      " [[0.         0.70710678 0.         0.         0.70710678 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "temp = tfidf.transform([\"dog and man are friends\"])\n",
    "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<font color=black><br>\n",
    "\n",
    "- https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/\n",
    "- https://github.com/practical-nlp/practical-nlp/blob/master/Ch3/02_Bag_of_Words.ipynb\n",
    "- https://github.com/practical-nlp/practical-nlp/blob/master/Ch3/04_TF_IDF.ipynb\n",
    "\n",
    "<br></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
